{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXMX8yuAyl55"
   },
   "source": [
    "# Welcome to CS 5242 **Homework 4**\n",
    "\n",
    "ASSIGNMENT DEADLINE â° : **19 Sept 2022** \n",
    "\n",
    "In this assignment, we have three parts:\n",
    "\n",
    "1. Implement some functions in CNNs from scratch *(3 Points)*\n",
    "2. Implement a CNN and train for CIFAR10 using PyTorch *(5 Points)*\n",
    "3. Discussion (parametes and flops for AlexNet) *(2 Points)*\n",
    "\n",
    "Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs. In this semester, we will use Colab to run our experiments.\n",
    "\n",
    "> In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
    "\n",
    "### **Grades Policy**\n",
    "\n",
    "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
    "\n",
    "### **Cautions**\n",
    "\n",
    "**DO NOT** use external libraries like PyTorch or TensorFlow in your implementation.\n",
    "\n",
    "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "### **Contact**\n",
    "\n",
    "Please feel free to contact us if you have any question about this homework or need any further information.\n",
    "\n",
    "Slack (Recommend): Shenggan Cheng\n",
    "\n",
    "TA Email: shenggan@comp.nus.edu.sg\n",
    "\n",
    "> If you have not join the slack group, you can click [here](https://join.slack.com/t/cs5242ay20222-oiw1784/shared_invite/zt-1eiv24k1t-0J9EI7vz3uQmAHa68qU0aw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLeZHcOVBp4U"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Start by running the cell below to set up all required software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIgu_q2HBg-E",
    "outputId": "34ea78d3-201e-46ef-ce01-d30701c25c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtXcchT5H2PH"
   },
   "source": [
    "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2Yodsn4H6CB",
    "outputId": "28d15d46-f3f2-44bf-b0d2-b313a5c97d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ce1587810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTpFBLKSkKI0"
   },
   "source": [
    "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
    "\n",
    "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
    "- Click `connect` on the top-right\n",
    "\n",
    "After connecting to one GPU, you can check its status using `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES8KOxziiYky",
    "outputId": "b5284af9-0828-4273-a259-a783b8f7fff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 25 07:41:04 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yrZD7DDExF4"
   },
   "source": [
    "Everything is ready, you can move on and ***Good Luck !*** ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm1f362vdRgF"
   },
   "source": [
    "## Implement functions in CNNs from scratch\n",
    "\n",
    "In this section, you need to implement some functions commonly used in CNNs, including convolution, pooling, etc. \n",
    "\n",
    "We will compare the computational results of your implemented version with those of pytorch, expecting that the error between the correct implementation and pytorch will be very small.\n",
    "\n",
    "NOTE: \n",
    "\n",
    "1. Implement these functions from scratch, **without** using any neural network libraries. Use linear algebra libraries in python is ok.\n",
    "\n",
    "2. The performance of the function is not included in this scoring, You just need to pay attention to the correctness of your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV3DXc2jgeg7"
   },
   "source": [
    "### Step 1\n",
    "Given a 32x32 pixels, 3 channels input, get a torch tensor with torch.randn()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3UxGJxTegq9O"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x = torch.randn(batch_size, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxnlbBnFw9wB"
   },
   "source": [
    "### Step 2\n",
    "\n",
    "For each following functions in the list, get the output tensor \"torch_xxx_out\" with input as x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OLQGhRbJgpIZ"
   },
   "outputs": [],
   "source": [
    "torch_max_pool = nn.MaxPool2d(kernel_size=2,\n",
    "                              stride=1,\n",
    "                              padding=0,\n",
    "                              dilation=1,\n",
    "                              return_indices=False,\n",
    "                              ceil_mode=False)\n",
    "torch_avg_pool = nn.AvgPool2d(kernel_size=2,\n",
    "                              stride=1,\n",
    "                              padding=0,\n",
    "                              ceil_mode=False,\n",
    "                              count_include_pad=True,\n",
    "                              divisor_override=None)\n",
    "torch_conv = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=6,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0,\n",
    "                       dilation=1,\n",
    "                       groups=1,\n",
    "                       bias=True,\n",
    "                       padding_mode='zeros')\n",
    "torch_norm = nn.BatchNorm2d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ri163mxZgvEm"
   },
   "outputs": [],
   "source": [
    "torch_sigmoid_out = torch.sigmoid(x, out=None)\n",
    "tmp_tensor = torch.randint(3, (batch_size,))\n",
    "torch_cross_entropy_out = F.cross_entropy(x[::, ::, 0, 0], tmp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PBzDAo2rgwmx"
   },
   "outputs": [],
   "source": [
    "torch_max_pool_out = torch_max_pool(x)\n",
    "torch_avg_pool_out = torch_avg_pool(x)\n",
    "torch_conv_out = torch_conv(x)\n",
    "torch_norm_out = torch_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6YBRP6Qgylg"
   },
   "source": [
    "### Step 3\n",
    "\n",
    "Implement these functions from scratch, without using any neural network libraries. Use linear algebra libraries in python is ok. Output your tensors as \"my_xxx_out\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CsO5I40fgzWY"
   },
   "outputs": [],
   "source": [
    "def my_pooling_2d(img2d, ker_size, pooling_func, stride=1, padding=0):\n",
    "    pad_img = np.pad(img2d, ((padding, padding), (padding, padding)), mode='constant')\n",
    "    img_h, img_w = pad_img.shape\n",
    "\n",
    "    out_h = int((img_h - ker_size) / stride) + 1\n",
    "    out_w = int((img_w - ker_size) / stride) + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(ker_size), ker_size)\n",
    "    i1 = np.repeat(np.arange(img_h - ker_size + 1, step=stride), out_w)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j0 = np.tile(np.arange(ker_size), ker_size)\n",
    "    j1 = np.tile(np.arange(img_w - ker_size + 1, step=stride), out_h)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    selected_img = pad_img[i, j].squeeze()\n",
    "    max_pool_out = pooling_func(selected_img, axis=0).reshape(out_h, out_w)\n",
    "    return max_pool_out\n",
    "  \n",
    "def my_max_pool(x, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
    "        kernel_size: size of the window to take a max over, \n",
    "        stride: stride of the window,\n",
    "        padding: implicit zero padding to be added on both sides,\n",
    "        \n",
    "    Return:\n",
    "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
    "    \"\"\"\n",
    "\n",
    "    # === Complete the code (0.5')\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    H_out = int((H_in - kernel_size + 2 * padding) / stride) + 1\n",
    "    W_out = int((W_in - kernel_size + 2 * padding) / stride) + 1\n",
    "    C_out = C_in\n",
    "\n",
    "    y = torch.empty((N, C_out, H_out, W_out), dtype=x.dtype)\n",
    "    for n in range(N):\n",
    "        for c in range(C_in):\n",
    "            y[n,c,:,:] = torch.tensor(my_pooling_2d(x[n,c,:,:], kernel_size, np.max, stride, padding))\n",
    "    return y\n",
    "    # === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nGbn6oQcg4pM"
   },
   "outputs": [],
   "source": [
    "def my_avg_pool(x, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
    "        kernel_size: size of the window, \n",
    "        stride: stride of the window,\n",
    "        padding: implicit zero padding to be added on both sides,\n",
    "        \n",
    "    Return:\n",
    "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
    "    \"\"\"\n",
    "\n",
    "    # === Complete the code (0.5')\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    H_out = int((H_in - kernel_size + 2 * padding) / stride) + 1\n",
    "    W_out = int((W_in - kernel_size + 2 * padding) / stride) + 1\n",
    "    C_out = C_in\n",
    "\n",
    "    y = torch.empty((N, C_out, H_out, W_out), dtype=x.dtype)\n",
    "    for n in range(N):\n",
    "        for c in range(C_in):\n",
    "            y[n, c, :, :] = torch.tensor(my_pooling_2d(x[n, c, :, :], kernel_size, np.mean, stride, padding))\n",
    "    return y\n",
    "    # === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9gsDytvKg5c1"
   },
   "outputs": [],
   "source": [
    "def my_conv_2d(img2d, kernel, stride=1, padding=0):\n",
    "    pad_img = np.pad(img2d, ((padding, padding), (padding, padding)), mode='constant')\n",
    "    img_h, img_w = pad_img.shape\n",
    "    k = kernel.detach().numpy()\n",
    "    ker_size, _ = k.shape\n",
    "\n",
    "    out_h = int((img_h - ker_size) / stride) + 1\n",
    "    out_w = int((img_w - ker_size) / stride) + 1\n",
    "\n",
    "    i0 = np.repeat(np.arange(ker_size), ker_size)\n",
    "    i1 = np.repeat(np.arange(img_h - ker_size + 1, step=stride), out_w)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j0 = np.tile(np.arange(ker_size), ker_size)\n",
    "    j1 = np.tile(np.arange(img_w - ker_size + 1, step=stride), out_h)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    selected_img = pad_img[i, j].squeeze()\n",
    "    conv = k.reshape(-1, ker_size * ker_size)@selected_img\n",
    "    return conv.reshape(out_h, out_w)\n",
    "\n",
    "def my_conv(x, in_channels, out_channels, kernel_size, stride, padding, weight, bias):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
    "        in_channels: number of channels in the input image, it is C_in;\n",
    "        out_channels: number of channels produced by the convolution;\n",
    "        kernel_size: size of onvolving kernel, \n",
    "        stride: stride of the convolution,\n",
    "        padding: implicit zero padding to be added on both sides of each dimension,\n",
    "        \n",
    "    Return:\n",
    "        y: torch tensor of size (N, C_out, H_out, W_out)\n",
    "    \"\"\"\n",
    "\n",
    "    # === Complete the code (0.5')\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    H_out = int((H_in - kernel_size + 2 * padding) / stride) + 1\n",
    "    W_out = int((W_in - kernel_size + 2 * padding) / stride) + 1\n",
    "    C_out = out_channels\n",
    "    y = torch.empty((N, C_out, H_out, W_out))\n",
    "    for n in range(N):\n",
    "        for c_out in range(C_out):\n",
    "            conv_2d_out = torch.zeros((H_out, W_out))\n",
    "            for c_in in range(C_in):\n",
    "                conv_2d_out += torch.tensor(my_conv_2d(x[n, c_in, :, :], weight[c_out, c_in, :, :], stride, padding))\n",
    "            y[n, c_out, :, :] = conv_2d_out + bias[c_out].detach().numpy()\n",
    "    return y\n",
    "    # === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8sX0oRyTg-m6"
   },
   "outputs": [],
   "source": [
    "def my_batchnorm(x, num_features, eps):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with size (N, C, H, W),\n",
    "        num_features: number of features in the input tensor, it is C;\n",
    "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
    "        \n",
    "    Return:\n",
    "        y: torch tensor of size (N, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    # === Complete the code (0.5')\n",
    "    y = torch.empty_like(x)\n",
    "    _, C, _, _ = x.shape\n",
    "    for c in range(C):\n",
    "        mean = np.mean(x[:,c,:,:].numpy())\n",
    "        variance = np.var(x[:,c,:,:].numpy())\n",
    "        d = (variance + eps) ** 0.5\n",
    "        y[:,c,:,:] = torch.tensor((x[:,c,:,:].numpy() - mean) / d)\n",
    "    return y\n",
    "    # === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZqwnIjPOhCQm"
   },
   "outputs": [],
   "source": [
    "def my_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch tensor with any size\n",
    "\n",
    "    Return:\n",
    "        y: the logistic sigmoid function of x\n",
    "    \"\"\"\n",
    "    # === Complete the code (0.5')\n",
    "    y = torch.empty_like(x)\n",
    "    N, C, _, _ = x.shape\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            y[n,c,:,:] = 1 / (1 + np.exp(-x[n,c,:,:]))\n",
    "    return y\n",
    "    # === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZPIFPl3ehEFh"
   },
   "outputs": [],
   "source": [
    "def my_cross_entropy(p, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p: torch tensor with size of (N, C),\n",
    "        y (int): torch tensor with size of (N), the values range from 0 to C-1\n",
    "\n",
    "    Return:\n",
    "        loss: the cross_entropy of predicted values p and target y.\n",
    "    \"\"\"\n",
    "    # === Complete the code (0.5')\n",
    "    loss = 0\n",
    "    N, C = p.shape\n",
    "    for n in range(N):\n",
    "        dividend = np.exp(p[n, y[n]].numpy())\n",
    "        divisor = np.sum(np.exp(p[n,:]).numpy())\n",
    "        loss -= np.log(dividend / divisor)\n",
    "    return torch.tensor(loss / N)\n",
    "    # === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dMnKzeVuhGxu"
   },
   "outputs": [],
   "source": [
    "my_max_pool_out = my_max_pool(x, kernel_size=2, stride=1, padding=0)\n",
    "my_avg_pool_out = my_avg_pool(x, kernel_size=2, stride=1, padding=0)\n",
    "my_conv_out = my_conv(x,\n",
    "                      in_channels=3,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=0,\n",
    "                      weight=torch_conv.weight,\n",
    "                      bias=torch_conv.bias)\n",
    "my_norm_out = my_batchnorm(x, num_features=3, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yUOZJ875hLAD"
   },
   "outputs": [],
   "source": [
    "my_sigmoid_out = my_sigmoid(x)\n",
    "my_cross_entropy_out = my_cross_entropy(x[::, ::, 0, 0], tmp_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jO-EHT7wm7Dk"
   },
   "source": [
    "### Step 4\n",
    "\n",
    "Compare and show that \"torch_xxx_out\" and \"my_xxx_out\" are equal up to small numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXnNfKKJhOAi",
    "outputId": "cf735e56-8ff8-4c07-b165-530907491a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(4.7220e-15, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9662e-15, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(F.mse_loss(my_max_pool_out, torch_max_pool_out))\n",
    "print(F.mse_loss(my_avg_pool_out, torch_avg_pool_out))\n",
    "print(F.mse_loss(my_conv_out, torch_conv_out))\n",
    "print(F.mse_loss(my_norm_out, torch_norm_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHxuCzkmhP4l",
    "outputId": "a83bd4c0-258b-4966-df10-921f4ffd5c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6253e-16)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(F.mse_loss(my_sigmoid_out, torch_sigmoid_out))\n",
    "print(F.mse_loss(my_cross_entropy_out, torch_cross_entropy_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJFZB_A7ddrC"
   },
   "source": [
    "## Train CNNs on CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88y6cXmPs7_E"
   },
   "source": [
    "Implement a CNN and train for CIFAR10 with these definitions:\n",
    "\n",
    "1. cA-B = Conv2d with input A channels, output B channels - kernel size 3x3, stride (1,1), padding with zeros to keep image size constant, followed by ReLU;\n",
    "\n",
    "2. mp = maxpool2d kernel size 2x2, stride (2,2);\n",
    "\n",
    "3. bn = batchnorm2d with affine=False (i.e. non learning batch norm);\n",
    "\n",
    "4. fcA-B = nn.linear with input A nodes, output B nodes;\n",
    "\n",
    "5. aap = adaptive average pooling.\n",
    "\n",
    "Use the definition to make the architecture c3-16 -> c16-16 -> mp -> c16-32 -> c32-32 -> mp -> c32-64 -> c64-64 -> mp -> c64-128 -> c128-128 -> aap -> flatten -> fc128-10 -> cross entropy loss. Adjust learning rate, batch size and other hyper parameters to make classification results **> 75%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HeUlgbGgg9JI"
   },
   "outputs": [],
   "source": [
    "# === Complete the code (1')\n",
    "num_epoch = 20 # TODO: please define the number of epoch here.\n",
    "batch_size = 128 # TODO: please fill the batch size here.\n",
    "# === Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "38b30bb8ba814821a872659ef9fa7e57",
      "db4bc6628c624ebb8ef2e8cbed30926d",
      "cdfbe1b8566d4d6ab90d32ff75b6e920",
      "6ce5af58d5ae4bf7ad59548e8fca27e2",
      "56dd9f5ade82407cae68ab1076ee25ce",
      "011f340b4aef405f9b8d40f6a7dea4eb",
      "a6c319657d474d7bbe529d68ba424076",
      "1ff16412d1864de8bfd35b87f847aff2",
      "f73ed3f0cc0e45028675c8b7fd568c07",
      "bdf681ad23a34a53ae22296a2b9a2faf",
      "a888ed1e4ac7414cac4b643f2252c773"
     ]
    },
    "id": "e9DLvGs6hmLJ",
    "outputId": "96e25fd1-49a8-4453-f898-565c4227707f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b30bb8ba814821a872659ef9fa7e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "o0VD5zmxhvkT"
   },
   "outputs": [],
   "source": [
    "# Creating a CNN model\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "       \n",
    "        # === Complete the code (1.5')\n",
    "        self.conv1 = self.__conv(3, 16)\n",
    "        self.conv2 = self.__conv(16, 16)\n",
    "        self.conv3 = self.__conv(16, 32)\n",
    "        self.conv4 = self.__conv(32, 32)\n",
    "        self.conv5 = self.__conv(32, 64)\n",
    "        self.conv6 = self.__conv(64, 64)\n",
    "        self.conv7 = self.__conv(64, 128)\n",
    "        self.conv8 = self.__conv(128, 128)\n",
    "\n",
    "        self.maxPooling = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.aap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "\n",
    "        # === Complete the code\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # === Complete the code (1.5')\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxPooling(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.maxPooling(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.maxPooling(x)\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = self.aap(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        out = self.fc(x)\n",
    "\n",
    "        # === Complete the code\n",
    "        return out\n",
    "    def __conv(self, in_channels, out_channels):\n",
    "        return nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Q8LfQbxThx_0"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN(10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYClpDydh0qA",
    "outputId": "892167f3-15a2-4173-a91c-7a2bd6e4890d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 |   128 batches loss: 2.1171\n",
      "epoch   0 |   256 batches loss: 1.8518\n",
      "epoch   0 |   384 batches loss: 1.6782\n",
      "epoch   1 |   128 batches loss: 1.5882\n",
      "epoch   1 |   256 batches loss: 1.5213\n",
      "epoch   1 |   384 batches loss: 1.4575\n",
      "epoch   2 |   128 batches loss: 1.4133\n",
      "epoch   2 |   256 batches loss: 1.3786\n",
      "epoch   2 |   384 batches loss: 1.3261\n",
      "epoch   3 |   128 batches loss: 1.2791\n",
      "epoch   3 |   256 batches loss: 1.2486\n",
      "epoch   3 |   384 batches loss: 1.2170\n",
      "epoch   4 |   128 batches loss: 1.1826\n",
      "epoch   4 |   256 batches loss: 1.1544\n",
      "epoch   4 |   384 batches loss: 1.1335\n",
      "epoch   5 |   128 batches loss: 1.0771\n",
      "epoch   5 |   256 batches loss: 1.0622\n",
      "epoch   5 |   384 batches loss: 1.0595\n",
      "epoch   6 |   128 batches loss: 1.0016\n",
      "epoch   6 |   256 batches loss: 0.9840\n",
      "epoch   6 |   384 batches loss: 0.9910\n",
      "epoch   7 |   128 batches loss: 0.9544\n",
      "epoch   7 |   256 batches loss: 0.9675\n",
      "epoch   7 |   384 batches loss: 0.9339\n",
      "epoch   8 |   128 batches loss: 0.8840\n",
      "epoch   8 |   256 batches loss: 0.8948\n",
      "epoch   8 |   384 batches loss: 0.8848\n",
      "epoch   9 |   128 batches loss: 0.8509\n",
      "epoch   9 |   256 batches loss: 0.8335\n",
      "epoch   9 |   384 batches loss: 0.8344\n",
      "epoch  10 |   128 batches loss: 0.8027\n",
      "epoch  10 |   256 batches loss: 0.8013\n",
      "epoch  10 |   384 batches loss: 0.8045\n",
      "epoch  11 |   128 batches loss: 0.7627\n",
      "epoch  11 |   256 batches loss: 0.7671\n",
      "epoch  11 |   384 batches loss: 0.7586\n",
      "epoch  12 |   128 batches loss: 0.7274\n",
      "epoch  12 |   256 batches loss: 0.7228\n",
      "epoch  12 |   384 batches loss: 0.7292\n",
      "epoch  13 |   128 batches loss: 0.6899\n",
      "epoch  13 |   256 batches loss: 0.6984\n",
      "epoch  13 |   384 batches loss: 0.6812\n",
      "epoch  14 |   128 batches loss: 0.6602\n",
      "epoch  14 |   256 batches loss: 0.6633\n",
      "epoch  14 |   384 batches loss: 0.6541\n",
      "epoch  15 |   128 batches loss: 0.6231\n",
      "epoch  15 |   256 batches loss: 0.6173\n",
      "epoch  15 |   384 batches loss: 0.6304\n",
      "epoch  16 |   128 batches loss: 0.5947\n",
      "epoch  16 |   256 batches loss: 0.5959\n",
      "epoch  16 |   384 batches loss: 0.6010\n",
      "epoch  17 |   128 batches loss: 0.5870\n",
      "epoch  17 |   256 batches loss: 0.5838\n",
      "epoch  17 |   384 batches loss: 0.5730\n",
      "epoch  18 |   128 batches loss: 0.5396\n",
      "epoch  18 |   256 batches loss: 0.5442\n",
      "epoch  18 |   384 batches loss: 0.5384\n",
      "epoch  19 |   128 batches loss: 0.5067\n",
      "epoch  19 |   256 batches loss: 0.5131\n",
      "epoch  19 |   384 batches loss: 0.5175\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # === Complete the code (1')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # === Complete the code\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 128 == 0:\n",
    "            print('epoch {:3d} | {:5d} batches loss: {:.4f}'.format(epoch, i + 1, running_loss/128))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "kuzhd2zWh415"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0_3yVTTh-U6",
    "outputId": "d6965e88-ad84-4383-ac8c-6c1ee92cbf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 76 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        #images, labels = data\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z53XxMASdm-A"
   },
   "source": [
    "## Discussion (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpbGzIo9dqWN"
   },
   "source": [
    "Calculate Parameters and FLOPs(Floating point operations) of **AlexNet** and analyse the ratio of the number of parameters and the amount of calculations for different layers in AlexNet.\n",
    "\n",
    "Hint:\n",
    "\n",
    "1. You can refer https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html for architecture of AlexNet.\n",
    "2. You only need to make estimates and do not need to perform rigorous calculations, (e.g. only consider the FLOPs of the convolution and FC in AlexNet model)\n",
    "3. Because Multiply Accumulate (MAC) operations are performed on the hardware, it is possible to simply consider only the number of multiplications when considering the number of operations when calculating FLOPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PB9Y3Ag5fPnV"
   },
   "source": [
    "| Layer                     | Input_size | Output_size | N(parameters)           | FLOPs                       | N(parameters) / FLOPs |\n",
    "| ------------------------- | ---------- | ----------- | ----------------------- | --------------------------- | --------------------- |\n",
    "| Conv2d(3, 64, 11, 4)      | 224        | 55          | 3Ã—64Ã—11Ã—11 + 64 = 23296 | 11Ã—11Ã—3Ã—64Ã—55Ã—55 = 70276800 | 0.00033148919         |\n",
    "| Conv2d(64,192,5,1)        | 28         | 24          | 307392                  | 176947200                   | 0.00173719618         |\n",
    "| Conv2d(192,384,3,1)       | 12         | 10          | 663936                  | 66355200                    | 0.01000578703         |\n",
    "| Conv2d(384,256,3,1)       | 10         | 8           | 884992                  | 56623104                    | 0.01562952112         |\n",
    "| Conv2d(256,256,3,1)       | 8          | 6           | 590080                  | 21233664                    | 0.0277898341          |\n",
    "| Linear(256 * 6 * 6, 4096) |            |             | 37752832                | 37748736                    | 1.00010850694         |\n",
    "| Linear(4096, 4096)        |            |             | 16781312                | 16777216                    | 1.00024414063         |\n",
    "| Linear(4096, 1000)        |            |             | 4097000                 | 4096000                     | 1.00024414063         |\n",
    "| **Total**                 |            |             | 60793448â‰ˆ60.8M          | 273110720                   |                       |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "011f340b4aef405f9b8d40f6a7dea4eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ff16412d1864de8bfd35b87f847aff2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38b30bb8ba814821a872659ef9fa7e57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db4bc6628c624ebb8ef2e8cbed30926d",
       "IPY_MODEL_cdfbe1b8566d4d6ab90d32ff75b6e920",
       "IPY_MODEL_6ce5af58d5ae4bf7ad59548e8fca27e2"
      ],
      "layout": "IPY_MODEL_56dd9f5ade82407cae68ab1076ee25ce"
     }
    },
    "56dd9f5ade82407cae68ab1076ee25ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ce5af58d5ae4bf7ad59548e8fca27e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdf681ad23a34a53ae22296a2b9a2faf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a888ed1e4ac7414cac4b643f2252c773",
      "value": " 170498071/170498071 [00:13&lt;00:00, 31878941.69it/s]"
     }
    },
    "a6c319657d474d7bbe529d68ba424076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a888ed1e4ac7414cac4b643f2252c773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdf681ad23a34a53ae22296a2b9a2faf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdfbe1b8566d4d6ab90d32ff75b6e920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ff16412d1864de8bfd35b87f847aff2",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f73ed3f0cc0e45028675c8b7fd568c07",
      "value": 170498071
     }
    },
    "db4bc6628c624ebb8ef2e8cbed30926d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_011f340b4aef405f9b8d40f6a7dea4eb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a6c319657d474d7bbe529d68ba424076",
      "value": "100%"
     }
    },
    "f73ed3f0cc0e45028675c8b7fd568c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
